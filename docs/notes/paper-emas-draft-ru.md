# YAAM: Суверенная, событийно-ориентированная архитектура памяти для корпоративных мультиагентных систем

**Аннотация**
Переход от чат-ботов к автономным агентам в корпоративных средах (Enterprise MAS) требует фундаментального пересмотра архитектуры памяти. Существующие решения часто ограничиваются эфемерным контекстом (RAG) или представляют собой монолитные платформы, что создает риски для приватности данных и усложняет аудит. В данной работе мы представляем **YAAM (Yet Another Agent Memory)** — эталонную микросервисную архитектуру для реализации долгосрочной памяти (System 2).
Ключевым вкладом работы является переход к **Архитектуре, основанной на навыках (Skill-Based Architecture)** в рамках парадигмы «Когнитивной Операционной Системы». Мы реализуем строгое разделение между Ядром (механизмом хранения) и Навыками (политикой использования), опираясь на полиглодную персистентность: **Redis** (L1), **PostgreSQL** (L2), **Qdrant** и **Neo4j** (L3), **Typesense** (L4). Мы формализуем Жизненный Цикл Знаний через асинхронные процессы Продвижения (Promotion) и Дистилляции (Distillation), использующие семантическую оценку важности событий. Валидация системы проведена на распределенной инфраструктуре с использованием адаптированного бенчмарка GoodAI. Результаты демонстрируют высокую скорость извлечения (<300ms) и архитектурную надежность, однако выявляют проблему «Разрыва между Поиском и Рассуждением» (Retrieval-Reasoning Gap) при использовании легковесных моделей.

---

## 1. Введение

Парадигма Искусственного Интеллекта смещается от дискретных взаимодействий к непрерывной автономной деятельности. В таких доменах, как управление цепями поставок (SCM), агенты должны поддерживать согласованность состояния на протяжении недель, а не минут, оперируя сложными сущностями: заказами, инцидентами, контрактами. Стандартные LLM страдают от «эффекта золотой рыбки» (отсутствия персистентности) и ограниченного окна контекста. Распространенное решение — Retrieval-Augmented Generation (RAG) — рассматривает память как статичную библиотеку, игнорируя динамическую, эволюционную природу агентного опыта.

Существующие инженерные решения (например, MemGPT или LangChain Memory) часто реализуют память как компонент, жестко связанный с **Runtime** агента. Такой подход (tight coupling) в корпоративных системах создает ряд ограничений: при перезапуске сервиса агента теряется состояние «рабочей памяти», а отсутствие строгой изоляции данных затрудняет аудит принятия решений. Более того, часто наблюдается зависимость от облачных векторных хранилищ, что неприемлемо для закрытых контуров (on-premise).

Мы предлагаем **YAAM**, архитектуру, которая:

1. Рассматривает память как **суверенную подсистему**, отделенную от логики агента через паттерн «Порты и Адаптеры» (Hexagonal Architecture).
2. Заменяет пассивное хранение активным **Жизненным Циклом Знаний**, где информация проходит через фильтры значимости и сжатия.
3. Вводит уровень абстракции **Навыков (Skills)**, позволяющий динамически изменять стратегии работы с данными без пересборки ядра системы.

---

## 2. Обзор связанных работ (Related Work)

В то время как классические RAG-системы фокусируются на точности семантического поиска (`top-k retrieval`), агентная память требует поддержки эпизодической хронологии и процедурных знаний.

* **MemGPT** вводит концепцию виртуальной памяти OS для LLM, управляя контекстным окном через системные вызовы. Однако монолитная природа решения затрудняет его интеграцию в распределенные мультиагентные системы (MAS).
* **Generative Agents (Park et al.)** демонстрируют важность рефлексии и графов памяти, но их архитектура, построенная на игровом цикле, не оптимизирована для latency-критичных бизнес-задач.
* **Наш подход:** Мы развиваем идеи иерархической памяти, но, в отличие от предшественников, формализуем разделение между *Механизмом* (хранение) и *Политикой* (навыки доступа). Это позволяет нам использовать специализированные промышленные СУБД для каждого типа памяти, обеспечивая производительность и надежность уровня Enterprise.

---

## 3. Архитектура YAAM

Мы концептуализируем YAAM не как библиотеку, а как **Когнитивную Операционную Систему (Cognitive OS)**, состоящую из трех функциональных блоков.

**[Figure 1: High-Level Hexagonal Architecture]**
*(На схеме показано Ядро с 4 уровнями памяти, окруженное Адаптерами к БД, и отдельный слой Навыков, подключаемый к Агенту)*

### 3.1. Компоненты Системы

1. **Kernel (YAAM Core):** Обеспечивает низкоуровневые интерфейсы I/O, управление транзакциями и фоновые процессы консолидации памяти.
2. **User Space (Skills Registry):** Библиотека отчуждаемых модулей, определяющих, *как* взаимодействовать с памятью и внешним миром.
3. **Executive Function (The Agent):** Сам LLM-агент, который, подобно планировщику процессов, выбирает и активирует необходимые Навыки. *(Примечание: Реализация Executive Function делегируется агенту, однако YAAM предоставляет необходимые метаданные для принятия решений)*.

#### Формализация: Tool vs. Skill

В соответствии с формирующимся консенсусом в инженерной психологии агентов [см. *arXiv:2601.04748*], мы проводим строгое разграничение:

* **Инструмент (Tool):** Атомарная, исполняемая единица функциональности (например, API-вызов `query_neo4j`). Инструмент агностичен к контексту.
* **Навык (Skill):** Когнитивная обертка. Навык включает в себя: (a) *Системный промпт* (инструкцию), (b) *Набор инструментов* (Tools), и (c) *Метаданные триггеров*. Навык является «компетенцией» агента.

### 3.2. Иерархия памяти и Полиглотная Персистентность

Мы отказались от попыток использовать одну базу данных для всех задач («Golden Hammer»). Вместо этого, каждый уровень иерархии (L1-L4) реализован на технологии, оптимально подходящей для его паттерна нагрузки:

1. **L1 (Active Context) — Redis:**
* *Роль:* "Сенсорная память". Хранит текущий диалог и эфемерный контекст.
* *Обоснование:* Критически важна низкая латентность (<5ms) и поддержка TTL (Time-to-Live) для автоматического забывания нерелевантного шума. Redis Streams также используется как шина событий для асинхронной обработки.


2. **L2 (Working Memory) — PostgreSQL:**
* *Роль:* "Оперативная память задач". Хранит состояние сессий, текущие цели (Goals) и списки дел (ToDo).
* *Обоснование:* Требуется строгая согласованность (ACID) и реляционная целостность данных. JSONB позволяет гибко хранить атрибуты задач.


3. **L3 (Episodic Memory) — Qdrant + Neo4j:**
* *Роль:* "Эпизодическая память". Хранит историю событий.
* *Обоснование:* Гибридный подход. **Qdrant** (Vector Store) обеспечивает быстрый семантический поиск по схожести (HNSW индексы). **Neo4j** (Graph DB) обеспечивает навигацию по связям (кто? с кем? когда?), что невозможно сделать эффективно через одни лишь векторы.


4. **L4 (Semantic Memory) — Typesense:**
* *Роль:* "Семантическая память". База знаний, документы, факты.
* *Обоснование:* Векторный поиск часто упускает точные совпадения (например, артикул детали "XJ-900"). **Typesense** обеспечивает быстрый полнотекстовый поиск (BM25) с поддержкой опечаток, дополняя векторный поиск.



### 3.3. Жизненный цикл знаний (The Knowledge Lifecycle)

Вместо синхронной обработки, блокирующей ответ пользователю, YAAM использует **Event-Driven Architecture** на базе Redis Streams.

**[Figure 2: Knowledge Lifecycle Pipeline]**
*(Схема показывает поток данных: User Input -> L1 -> Promotion Engine -> L2 -> Consolidation/Distillation -> L3/L4)*

#### Promotion Engine (Механизм Продвижения)

Этот движок отвечает за фильтрацию шума. Он анализирует входящий поток сообщений из L1, применяя логику **CIAR (Context-Item-Action-Result)**.

* *Семантический Скоринг:* LLM выступает в роли классификатора, оценивая каждый факт по шкале значимости (0.0 - 1.0).
* *Математическая Логика:* Только факты, превысившие порог значимости (Significance Threshold) и содержащие новую информацию (Delta), "продвигаются" (promoted) в уровень L2. Это предотвращает засорение долгосрочной памяти приветствиями и техническим шумом.

#### Distillation Engine (Механизм Дистилляции)

Работает как фоновый процесс (Background Worker) для формирования L3/L4.

* *Сжатие:* Завершенные эпизоды из L2 агрегируются и суммаризируются.
* *Экстракция:* Из текста извлекаются сущности и связи, которые затем записываются в граф (Neo4j).
* *Кристаллизация:* Повторяющиеся паттерны поведения обобщаются в абстрактные правила, сохраняемые в L4.

---

## 4. Реализация и Инженерия

Архитектура построена на принципах **Cloud-Native** и **Reproducibility**.

**[Figure 3: Deployment & Infrastructure]**
*(Диаграмма Docker-контейнеров, показывающая изоляцию сервисов и volumes)*

* **Изоляция Контейнеров:** Runtime агента (FastAPI) и State (БД) разделены на уровне контейнеров Docker. Это гарантирует, что агент не может «сломать» память прямым доступом к файлам (Leakage prevention).
* **Изоляция Тестирования (Clean Room):** Критически важным аспектом реализации является физическое разделение сервиса агента и сервиса бенчмарка (Benchmark Runner). Они взаимодействуют исключительно через внешнее API. Это обеспечивает академическую чистоту эксперимента: агент не имеет доступа к тестовым данным (Test Set) во время своей работы, исключая возможность "Train on Test".
* **Суверенитет:** Поддержка Docker Profiles (`local-db`) позволяет развернуть полный стек внутри изолированного контура предприятия, без отправки данных на внешние сервера (кроме вызовов LLM, если используется не локальная модель).

---

## 5. Валидация и Результаты

### 5.1. Методология

Для оценки системы мы адаптировали бенчмарк **GoodAI LTM Benchmark**. Оригинальный код был переписан для работы в режиме "Black Box" через HTTP, что позволяет тестировать удаленных агентов, имитируя реальные сетевые задержки и асинхронность ответов.

**[Figure 4: Evaluation Protocol]**
*(Схема взаимодействия Benchmark Runner и YAAM Agent через API)*

### 5.2. Результаты производительности

* **Latency:** Извлечение контекста из L1 занимает <5ms. Сложный поиск по графу L3 занимает 250-400ms, однако благодаря асинхронности, агент может начать генерировать ответ, используя L1/L2, подгружая данные из L3 по мере готовности (streaming).
* **Token Efficiency:** Использование механизма *Skills* (динамическая подгрузка промптов) сократило потребление токенов в системном контексте на **60%** по сравнению с монолитной загрузкой всех схем.

### 5.3. Разрыв между Поиском и Рассуждением (The Retrieval-Reasoning Gap)

Эксперименты проводились с использованием модели **Google Gemini 2.5 Flash-Lite**.
Наши тесты выявили феномен, который мы называем **Retrieval-Reasoning Gap** (созвучно с *Long-Context Paradox* [Liu et al., 2024]).

* *High Recall:* Архитектура YAAM успешно обеспечивала Recall >95%, находя релевантные факты в графе L3 и помещая их в контекст.
* *Reasoning Failure:* Однако, модель класса "Flash-Lite" часто игнорировала эти факты при генерации ответа, если они противоречили её внутренним "галлюцинациям" или требовали многоступенчатого логического вывода.

*Примечание:* Эксперименты с более мощными моделями (Gemini 1.5 Pro, GPT-4o) находятся в процессе проведения. Предварительные данные указывают на то, что более мощные модели способны эффективно закрывать этот разрыв, корректно используя предоставляемый YAAM контекст.

---

## 6. Заключение

YAAM демонстрирует переход от архитектуры «Database Connector» к архитектуре «Cognitive OS». Мы показали, что интеграция специализированных Open-Source решений (Redis, Postgres, Neo4j, Qdrant, Typesense) под управлением асинхронных когнитивных движков позволяет создавать суверенные, расширяемые системы.

Выявленный "Разрыв между Поиском и Рассуждением" подтверждает гипотезу, что внешняя память является **мультипликатором интеллекта**, но не его заменой. Будущие исследования будут сосредоточены на совершенствовании **Executive Function** агента — разработке мета-когнитивных стратегий для более эффективного выбора навыков.

**Репозиторий:** [https://github.com/maksim-tsi/yaam-agent-memory]