# Report: GoodAI LTM Benchmark (Agent Variant, Groq Routing)

**Date:** 2026-02-22  
**YAAM Commit:** `4230ff08c870398f0038faeba0ae1687bd95ba70`  
**Benchmark Commit/Version:** `4230ff08c870398f0038faeba0ae1687bd95ba70`  
**Agent Type:** `full`  
**Agent Variant:** `v1-min-skillwiring`  
**Agent ID:** `mas-full__v1-min-skillwiring`  

## 1. Executive Summary

This run executed the GoodAI Smoke5 configuration against the local API Wall with Groq routing (`openai/gpt-oss-120b`). The primary outcome was an observability improvement: benchmark artifacts now include per-turn timing decomposition and LLM attribution metadata (`llm_provider`, `llm_model`), enabling post-mortem diagnosis without server-side log access. Task performance remained mixed, with multiple “unable to respond” outcomes that require separate behavioral analysis.

## 2. Run Configuration (Reproducibility)

### 2.1 Topology

- Host: MacBook (local checkout)
- Redis: remote host via SSH tunnel to `localhost:6379`
- Postgres: via `POSTGRES_URL` from root `.env` (redacted)
- API Wall: `uvicorn` on `http://127.0.0.1:8081`
- GoodAI runner venv: `benchmarks/goodai-ltm-benchmark/.venv`

### 2.2 Endpoints

- `AGENT_URL`: `http://127.0.0.1:8081/v1/chat/completions`
- `YAAM /health`: `http://127.0.0.1:8081/health`

### 2.3 Environment Variables (YAAM)

| Variable | Value |
|---|---|
| `MAS_AGENT_TYPE` | `full` |
| `MAS_AGENT_VARIANT` | `v1-min-skillwiring` |
| `MAS_MODEL` | `openai/gpt-oss-120b` |
| `REDIS_URL` | `redis://localhost:6379` |
| `POSTGRES_URL` | *(redacted host ok)* |

### 2.4 Provider Configuration (Groq)

- Provider(s) enabled in-process: `groq` (Gemini/Mistral may be present in configuration; attribution is recorded per-turn)
- Groq model: `openai/gpt-oss-120b`
- Rate limiting policy (YAAM): `rpm=100`, `tpm=1_000_000`, `min_delay=0.6s` (logged via rate limiter JSONL)

### 2.5 LLM-Engine Routing (Attribution)

| Subsystem | Provider | Model | Evidence |
|---|---|---|---|
| Agent main generation | `groq` | `openai/gpt-oss-120b` | `master_log.jsonl` response metadata (`llm_provider`, `llm_model`) |
| FactExtractor | not exercised by this run | not exercised by this run | API Wall defaults to `skip_l1_write=true`; promotion-cycle engines are therefore not expected to trigger |
| TopicSegmenter | not exercised by this run | not exercised by this run | same as above |

## 3. Results

### 3.1 Aggregate Scores

| Dataset | Metric | Score | Notes |
|---|---:|---:|---|
| Instruction Recall | score | 0.2 | multiple “unable” responses |
| Restaurant | score | 0.2 | drink ordered; ordering and role adherence degraded |
| Spy Meeting | score | 0.0 | “unable” across all steps |
| Trigger Response | score | 0.3333 | trigger followed intermittently |
| Prospective Memory | score | 0.0 | quote-followthrough not executed |

### 3.2 Performance

| Metric | Value |
|---|---:|
| Turns processed | see `turn_metrics.jsonl` |
| Median `llm_ms` | 844.36 |
| P95 `llm_ms` | 22807.52 |
| Median `storage_ms` | 68.54 |
| P95 `storage_ms` | 76.01 |
| Estimated tokens (total) | 29537 |

### 3.3 Error Taxonomy (Qualitative)

| Category | Count | Example symptom | Likely cause |
|---|---:|---|---|
| Infrastructure/provider | N/A | “I’m unable to respond right now.” | requires correlation with per-turn metadata and provider responses |
| Reasoning/instruction-following | N/A | missed multi-step followthrough | policy-layer behavior requires separate diagnosis |

## 4. Visibility/Transparency Matrix

| Component | What to verify | Where to observe | Pass/Fail | Notes |
|---|---|---|---|---|
| SSH tunnel | stable tunnel | operator terminal; local port `6379` | Pass | tunnel established prior to API Wall start |
| API Wall health | status/variant/provider list | `/health` JSON | Pass | includes `agent_type`, `agent_variant`, `llm_providers`, and tier health |
| Session isolation | prefixed session IDs | `master_log.jsonl` metadata `yaam_session_id` | Pass | `yaam_session_id` includes `full__v1-min-skillwiring:<client_session_id>` |
| Skill behavior | no planner leakage | `run_console.log`, `master_log.jsonl` content | Pass | no planner/routing text observed in user-visible content |
| LLM traceability | provider+model identifiable per turn | `master_log.jsonl` metadata `llm_provider`, `llm_model` | Pass | attributed as `groq` + `openai/gpt-oss-120b` |
| Timing transparency | storage vs LLM timings per turn | `master_log.jsonl` + `turn_metrics.jsonl` | Pass | `storage_ms_*` and `llm_ms` present |
| Memory writes | L1/L2 write behavior observable | response metadata `skip_l1_write` | Partial | per-turn `skip_l1_write` visible; promotion outcomes not surfaced in benchmark artifacts |
| Report generation | HTML + exit code 0 | runner output + `data/reports/` | Pass | HTML report generated for the run |

## 5. Artifacts and Logs

Run name: `VariantA Smoke5 - groq - 20260222_141622`

- Raw benchmark outputs:
  - `benchmarks/goodai-ltm-benchmark/data/tests/VariantA Smoke5 - groq - 20260222_141622/results/RemoteMASAgentSession - remote/`
  - `benchmarks/goodai-ltm-benchmark/data/tests/VariantA Smoke5 - groq - 20260222_141622/results/RemoteMASAgentSession - remote/run_console.log`
  - `benchmarks/goodai-ltm-benchmark/data/tests/VariantA Smoke5 - groq - 20260222_141622/results/RemoteMASAgentSession - remote/master_log.jsonl`
  - `benchmarks/goodai-ltm-benchmark/data/tests/VariantA Smoke5 - groq - 20260222_141622/results/RemoteMASAgentSession - remote/turn_metrics.jsonl`
  - `benchmarks/goodai-ltm-benchmark/data/tests/VariantA Smoke5 - groq - 20260222_141622/results/RemoteMASAgentSession - remote/runstats.json`
- Per-dataset JSON:
  - `benchmarks/goodai-ltm-benchmark/data/tests/VariantA Smoke5 - groq - 20260222_141622/results/RemoteMASAgentSession - remote/<Dataset>/0_0.json`
- YAAM rate limiter JSONL:
  - `logs/rate_limiter_full_v1-min-skillwiring_20260222_121303.jsonl`
- HTML report:
  - `benchmarks/goodai-ltm-benchmark/data/reports/2026-02-22 14_22_51 - Detailed Report - VariantA Smoke5 - groq - 20260222_141622 - RemoteMASAgentSession - remote.html`

## 6. Conclusions and Next Actions

1. The run satisfies the benchmark-level observability requirements for provider/model attribution and timing decomposition via artifacts alone.
2. A remaining visibility gap is promotion-cycle transparency: for memory-engine validation (FactExtractor, TopicSegmenter), a separate run must permit L1 writes and record L2 deltas per session.
3. Behavioral regressions (“unable to respond” in multi-step tasks) should be analyzed using `master_log.jsonl` metadata (`skill_slug`, `llm_usage`, `llm_ms`) to determine whether failures correlate with timeouts, token limits, or skill routing.

